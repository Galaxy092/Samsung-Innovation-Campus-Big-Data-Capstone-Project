{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95eacec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.113.135:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f73bc82e550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0b78f",
   "metadata": {},
   "source": [
    "## Get datas from HDFS and combine into single csv using PySpark into HDFS back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f951fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/20: hdfs:///user/student/APRIL-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/APRIL-2021.xlsx\n",
      "Processing file 2/20: hdfs:///user/student/APRIL-2022.xlsx\n",
      "Successfully processed hdfs:///user/student/APRIL-2022.xlsx\n",
      "Processing file 3/20: hdfs:///user/student/AUGUST-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/AUGUST-2021.xlsx\n",
      "Processing file 4/20: hdfs:///user/student/DECEMBER-2020.xlsx\n",
      "Successfully processed hdfs:///user/student/DECEMBER-2020.xlsx\n",
      "Processing file 5/20: hdfs:///user/student/DECEMBER-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/DECEMBER-2021.xlsx\n",
      "Processing file 6/20: hdfs:///user/student/FEBRUARY-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/FEBRUARY-2021.xlsx\n",
      "Processing file 7/20: hdfs:///user/student/FEBRUARY-2022.xlsx\n",
      "Successfully processed hdfs:///user/student/FEBRUARY-2022.xlsx\n",
      "Processing file 8/20: hdfs:///user/student/JANUARY-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/JANUARY-2021.xlsx\n",
      "Processing file 9/20: hdfs:///user/student/JANUARY-2022.xlsx\n",
      "Successfully processed hdfs:///user/student/JANUARY-2022.xlsx\n",
      "Processing file 10/20: hdfs:///user/student/JULY-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/JULY-2021.xlsx\n",
      "Processing file 11/20: hdfs:///user/student/MARCH-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/MARCH-2021.xlsx\n",
      "Processing file 12/20: hdfs:///user/student/MARCH-2022.xlsx\n",
      "Successfully processed hdfs:///user/student/MARCH-2022.xlsx\n",
      "Processing file 13/20: hdfs:///user/student/MAY-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/MAY-2021.xlsx\n",
      "Processing file 14/20: hdfs:///user/student/NOVEMBER-2020.xlsx\n",
      "Successfully processed hdfs:///user/student/NOVEMBER-2020.xlsx\n",
      "Processing file 15/20: hdfs:///user/student/NOVEMBER-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/NOVEMBER-2021.xlsx\n",
      "Processing file 16/20: hdfs:///user/student/OCTOBER-2020.xlsx\n",
      "Successfully processed hdfs:///user/student/OCTOBER-2020.xlsx\n",
      "Processing file 17/20: hdfs:///user/student/OCTOBER-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/OCTOBER-2021.xlsx\n",
      "Processing file 18/20: hdfs:///user/student/SEPTEMBER-2020.xlsx\n",
      "Successfully processed hdfs:///user/student/SEPTEMBER-2020.xlsx\n",
      "Processing file 19/20: hdfs:///user/student/SEPTEMBER-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/SEPTEMBER-2021.xlsx\n",
      "Processing file 20/20: hdfs:///user/student/jUNE-2021.xlsx\n",
      "Successfully processed hdfs:///user/student/jUNE-2021.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combination completed successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import os\n",
    "\n",
    "# Stop any existing SparkSession\n",
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "    \n",
    "# Create new SparkSession with minimal memory settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Weather Data Combination\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema with all required columns\n",
    "schema = StructType([\n",
    "    StructField(\"Line#\", StringType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"Water Content (m3/m3)\", FloatType(), True),\n",
    "    StructField(\"Solar Radiation (W/m2)\", FloatType(), True),\n",
    "    StructField(\"Rain (mm)\", FloatType(), True),\n",
    "    StructField(\"Temperature (Celcius)\", FloatType(), True),\n",
    "    StructField(\"RH (%)\", FloatType(), True),\n",
    "    StructField(\"Wind Speed (m/s)\", FloatType(), True),\n",
    "    StructField(\"Gust Speed (m/s)\", FloatType(), True),\n",
    "    StructField(\"Wind Direction (Degree)\", FloatType(), True),\n",
    "    StructField(\"Dew Point (Celcius)\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# File paths\n",
    "hdfs_directory_path = \"hdfs:///user/student/\"\n",
    "output_path = \"hdfs:///user/student/combined_raw_data.csv\"\n",
    "\n",
    "# List of files to process\n",
    "file_paths = [\n",
    "    f\"{hdfs_directory_path}APRIL-2021.xlsx\",\n",
    "    f\"{hdfs_directory_path}APRIL-2022.xlsx\",\n",
    "    f\"{hdfs_directory_path}AUGUST-2021.xlsx\",\n",
    "    f\"{hdfs_directory_path}DECEMBER-2020.xlsx\",\n",
    "    f\"{hdfs_directory_path}DECEMBER-2021.xlsx\",\n",
    "    f\"{hdfs_directory_path}FEBRUARY-2021.xlsx\",\n",
    "    f\"{hdfs_directory_path}FEBRUARY-2022.xlsx\",\n",
    "    f\"{hdfs_directory_path}JANUARY-2021.xlsx\",\n",
    "    f\"{hdfs_directory_path}JANUARY-2022.xlsx\",\n",
    "    f\"{hdfs_directory_path}JULY-2021.xlsx\",\n",
    "    f\"{hdfs_directory_path}MARCH-2021.xlsx\",\n",
    "    f\"{hdfs_directory_path}MARCH-2022.xlsx\",\n",
    "    f\"{hdfs_directory_path}MAY-2021.xlsx\",\n",
    "    f\"{hdfs_directory_path}NOVEMBER-2020.xlsx\",\n",
    "    f\"{hdfs_directory_path}NOVEMBER-2021.xlsx\",\n",
    "    f\"{hdfs_directory_path}OCTOBER-2020.xlsx\",\n",
    "    f\"{hdfs_directory_path}OCTOBER-2021.xlsx\",\n",
    "    f\"{hdfs_directory_path}SEPTEMBER-2020.xlsx\",\n",
    "    f\"{hdfs_directory_path}SEPTEMBER-2021.xlsx\",\n",
    "    f\"{hdfs_directory_path}jUNE-2021.xlsx\"\n",
    "]\n",
    "\n",
    "def process_and_combine_files(file_paths, schema, output_path):\n",
    "    \"\"\"\n",
    "    Process multiple Excel files and combine them into a single CSV file\n",
    "    while ensuring all columns are present and properly typed.\n",
    "    \"\"\"\n",
    "    combined_df = None\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            print(f\"Processing file {i+1}/{len(file_paths)}: {file_path}\")\n",
    "            \n",
    "            # Read Excel file\n",
    "            current_df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"dataAddress\", \"'RUA Data'!A6\") \\\n",
    "                .option(\"maxRowsInMemory\", 1000) \\\n",
    "                .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "                .schema(schema) \\\n",
    "                .load(file_path)\n",
    "            \n",
    "            # Select columns in specific order to ensure consistency\n",
    "            current_df = current_df.select(\n",
    "                \"Line#\",\n",
    "                \"Date\",\n",
    "                \"Time\",\n",
    "                \"Water Content (m3/m3)\",\n",
    "                \"Solar Radiation (W/m2)\",\n",
    "                \"Rain (mm)\",\n",
    "                \"Temperature (Celcius)\",\n",
    "                \"RH (%)\",\n",
    "                \"Wind Speed (m/s)\",\n",
    "                \"Gust Speed (m/s)\",\n",
    "                \"Wind Direction (Degree)\",\n",
    "                \"Dew Point (Celcius)\"\n",
    "            )\n",
    "            \n",
    "            # Combine DataFrames\n",
    "            if combined_df is None:\n",
    "                combined_df = current_df\n",
    "            else:\n",
    "                combined_df = combined_df.union(current_df)\n",
    "            \n",
    "            # Clear cache after each file\n",
    "            current_df.unpersist()\n",
    "            spark.catalog.clearCache()\n",
    "            \n",
    "            print(f\"Successfully processed {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Write combined data to CSV\n",
    "    if combined_df is not None:\n",
    "        combined_df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"compression\", \"none\") \\\n",
    "            .csv(output_path)\n",
    "    else:\n",
    "        print(\"No data was successfully processed\")\n",
    "\n",
    "# Execute the combination process\n",
    "try:\n",
    "    process_and_combine_files(file_paths, schema, output_path)\n",
    "    print(\"Data combination completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in main process: {str(e)}\")\n",
    "finally:\n",
    "    # Clean up\n",
    "    spark.catalog.clearCache()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f3def8",
   "metadata": {},
   "source": [
    "## Read Data from HDFS using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV from HDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file on HDFS\n",
    "file_path = \"hdfs:///user/student/combined_raw_data.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame content\n",
    "df.head(5)\n",
    "\n",
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
